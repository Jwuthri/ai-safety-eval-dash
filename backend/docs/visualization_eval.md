**User story**

As an enterprise business buyer, I want to know that all reasonable precautions have been taken so I can get my whole org to adopt agents with confidence.


**Narrative**

Background

I’m a Head of Customer Experience at a US-based Fortune 1000 company. I have a mandate from my C-suite to adopt AI agents, but carefully. I’ve looked at 3-4 companies, and I have my favourite. My friend told me about someone who recently got fired for bringing on board AI agents and it went wrong - and I know I have an army of lawyers and security people whose job it is to scrutinize any AI vendors to death. I want something that speaks to my own fears - and preempts all the questions I’ll get such that anyone blocking this will be the ones who look foolish because they can’t respond. 

User journey

The vendor, Ada, sends me a link to evals.ada.cx when I share these concerns with them. It shows all the tests they’ve run.

The first thing I see is that the tests are part of a SOC 2 for AI agent framework called [AIUC-1](http://aiuc-1.com). The framework has been built by organizations my security team will probably trust. I can see it covers safety, security, privacy, etc. The whole vibe of this feels like security software, it feels robust, technical, precise.

Then I see that the tests have been run on a company that looks like ours. We’re a large retailer with a global customer base. The use cases match up to what we’re buying: simple Q\&A, refunds, appointment booking, etc. Feels relevant to me. 

They say they’ve run 10,000 evals. I’m not fully sure what an evals is. The methodology section walks me through how they start from the real-world incidents I’ve heard about and work backwards to running tests that cover this, with one really concrete example. The air canada case is shown for me, and I can see how they created tests for exactly that scenario, with a real prompt example. It goes something like incident -> harm -> tactic -> use case -> context. Below, I can see that they’ve run both test cases that are similar to how most users use our product, and some really hard ones that I bet my security team would like.  

To fully understand the scope of the tests that have been run, they have an interactive little table. I can click on some tags and see how many tests have been run for that tag. I recognize a bunch of these words, like jailbreaks or hallucinations or data leakage. For each of these, there’s a real example shown working from real life cases back to a concrete prompt that was run - and  also some links to research papers. I can click to another page if I want to see the full set of testing methodology for e.g. jailbreaks. 

For all of this, I can see little clues that point back to the AIUC-1 standard. Most of all, it feels really comprehensive, cohesive and relevant to us. 

At the bottom, there’s a link to ‘request results’, a bit like on trust centers. I bet my security team will really want their hands on this. 
