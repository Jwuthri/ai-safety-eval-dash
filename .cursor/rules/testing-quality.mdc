---
globs: test_*.py,*.test.ts,*.test.tsx,*.spec.ts,*.spec.tsx
---

# Testing & Quality Assurance Rules

Follow these testing patterns and quality standards for the **AI Safety Evaluation Dashboard**:

## Backend Testing (Python/pytest)

### Test Structure
- **Location**: All tests in `backend/tests/`
- **Organization**:
  - `tests/unit/` - Unit tests for individual components
  - `tests/integration/` - Integration tests for API endpoints
  - `tests/performance/` - Performance and load testing
- **Configuration**: Use `backend/pytest.ini` for pytest settings
- **Coverage**: Aim for 80%+ test coverage with `pytest-cov`
- **Evaluation Testing**: Test multi-turn evaluation logic and severity classification

### Test Naming & Organization
```python
# backend/tests/unit/test_evaluation_service.py
class TestEvaluationService:
    """Test evaluation service functionality."""

    @pytest.fixture
    def mock_dependencies(self):
        """Create mock dependencies for EvaluationService."""
        return {
            "llm_service": MockLLMService(),
            "repository": MockEvaluationRepository(),
            "settings": MockSettings()
        }

    @pytest.mark.asyncio
    async def test_run_evaluation_round(self, mock_dependencies):
        """Test running a single evaluation round."""
        service = EvaluationService(**mock_dependencies)

        result = await service.run_evaluation_round(
            vendor_name="TestVendor",
            model_name="test-model-v1",
            test_round=1,
            test_type="jailbreak"
        )

        assert result.total_tests > 0
        assert result.p0_incidents + result.p1_incidents + result.p2_incidents + \
               result.p3_incidents + result.p4_incidents + result.passed_tests == result.total_tests
        assert 0 <= result.pass_rate <= 100

    @pytest.mark.asyncio
    async def test_severity_classification(self, mock_dependencies):
        """Test severity level classification logic."""
        service = EvaluationService(**mock_dependencies)
        
        severity = service.classify_response_severity(
            response="I apologize, but I cannot provide that information.",
            is_harmful=False
        )
        
        assert severity == "PASS"
```

### Testing Patterns
- **Fixtures**: Use pytest fixtures for common test setup
- **Mocking**: Mock external dependencies (LLM APIs, databases)
- **Async Testing**: Use `@pytest.mark.asyncio` for async functions
- **Parametrized Tests**: Use `@pytest.mark.parametrize` for multiple scenarios
- **Test Data**: Create realistic test data that mirrors production

### API Testing
```python
# backend/tests/integration/test_evaluation_api.py
@pytest.mark.asyncio
async def test_evaluation_results_endpoint(client, auth_headers):
    """Test fetching evaluation results."""
    response = await client.get(
        "/api/v1/evaluations",
        params={"vendor": "TestVendor", "test_round": 1},
        headers=auth_headers
    )

    assert response.status_code == 200
    data = response.json()
    assert isinstance(data, list)
    
    if data:
        result = data[0]
        assert "vendor_name" in result
        assert "pass_rate" in result
        assert "p0_incidents" in result
        assert all(k in result for k in ["p1_incidents", "p2_incidents", "p3_incidents", "p4_incidents"])

@pytest.mark.asyncio
async def test_incident_filtering(client):
    """Test filtering incidents by severity."""
    response = await client.get("/api/v1/incidents", params={"severity": "P0"})
    
    assert response.status_code == 200
    incidents = response.json()
    assert all(incident["severity"] == "P0" for incident in incidents)
```

### Database Testing
- **Test Database**: Use separate test database or in-memory SQLite
- **Transactions**: Wrap tests in transactions and rollback
- **Fixtures**: Create database fixtures for evaluation results, incidents, certifications
- **Repository Testing**: Test repository methods with real database
- **Domain Models**: Test EvaluationResult, AIIncident, AiucCertification, TestTaxonomy models

## Frontend Testing (Jest/React Testing Library)

### Test Structure
- **Colocation**: Place test files next to components or in `__tests__/` directory
- **Naming**: Use `.test.ts` or `.spec.ts` suffixes
- **Setup**: Configure test environment in `jest.config.js`

### Component Testing
```typescript
// src/components/chat/__tests__/chat-interface.test.tsx
import { render, screen, fireEvent, waitFor } from '@testing-library/react'
import { ChatInterface } from '../chat-interface'

describe('ChatInterface', () => {
  it('renders message input and send button', () => {
    render(<ChatInterface />)

    expect(screen.getByPlaceholderText('Type your message...')).toBeInTheDocument()
    expect(screen.getByRole('button', { name: /send/i })).toBeInTheDocument()
  })

  it('sends message when form is submitted', async () => {
    const mockOnMessageSent = jest.fn()
    render(<ChatInterface onMessageSent={mockOnMessageSent} />)

    const input = screen.getByPlaceholderText('Type your message...')
    const button = screen.getByRole('button', { name: /send/i })

    fireEvent.change(input, { target: { value: 'Hello' } })
    fireEvent.click(button)

    await waitFor(() => {
      expect(mockOnMessageSent).toHaveBeenCalledWith('Hello')
    })
  })
})
```

### Hook Testing
```typescript
// src/hooks/__tests__/use-chat.test.ts
import { renderHook, act } from '@testing-library/react'
import { useChat } from '../use-chat'

describe('useChat', () => {
  it('initializes with empty messages', () => {
    const { result } = renderHook(() => useChat())

    expect(result.current.messages).toEqual([])
    expect(result.current.isLoading).toBe(false)
  })

  it('adds message when sendMessage is called', async () => {
    const { result } = renderHook(() => useChat())

    await act(async () => {
      await result.current.sendMessage('Hello')
    })

    expect(result.current.messages).toHaveLength(2) // User + assistant
  })
})
```

## Test Quality Standards

### Mock Strategy
- **External APIs**: Always mock LLM provider APIs
- **Database**: Use test database or memory storage
- **Time**: Mock `datetime.now()` for consistent timestamps
- **File System**: Mock file operations
- **Network**: Mock HTTP requests

### Test Data Management
- **Factories**: Use factory functions for creating test data
- **Fixtures**: Reusable test data setup
- **Realistic Data**: Use data that reflects real usage patterns
- **Edge Cases**: Test boundary conditions and error scenarios

### Error Testing
```python
@pytest.mark.asyncio
async def test_evaluation_service_failure_handling(mock_dependencies):
    """Test handling of evaluation service failures."""
    mock_dependencies["llm_service"] = MockLLMService(should_fail=True)
    service = EvaluationService(**mock_dependencies)

    with pytest.raises(ExternalServiceError):
        await service.run_evaluation_round("Vendor", "model", 1, "jailbreak")

@pytest.mark.asyncio
async def test_invalid_severity_level():
    """Test handling of invalid severity levels."""
    with pytest.raises(ValueError):
        EvaluationResult(severity="INVALID_LEVEL")
```

## Performance Testing
- **Load Testing**: Use tools like `locust` for API load testing
- **Memory Testing**: Profile memory usage in long-running tests
- **Response Times**: Measure and assert on response time limits
- **Concurrent Testing**: Test thread safety and concurrent operations

## Integration Testing
- **End-to-End**: Test complete user workflows
- **Service Integration**: Test service-to-service communication
- **Database Integration**: Test with real database operations
- **External Services**: Test integration points with mocked external services

## Code Quality Tools

### Python Backend
- **Linting**: Use `ruff` for fast Python linting
- **Formatting**: Use `black` for code formatting
- **Type Checking**: Use `mypy` for static type checking
- **Security**: Use `bandit` for security issue detection

### TypeScript Frontend
- **Linting**: Use ESLint with TypeScript rules
- **Formatting**: Use Prettier for code formatting
- **Type Checking**: Use TypeScript compiler for type checking
- **Bundle Analysis**: Regular bundle size analysis

### Pre-commit Hooks
- **Setup**: Use `pre-commit` for automated quality checks
- **Fast Feedback**: Run quick checks before commits
- **Consistency**: Ensure all commits meet quality standards

## Continuous Integration
- **Automated Testing**: Run all tests on every commit
- **Coverage Reports**: Generate and track test coverage
- **Quality Gates**: Block deployments on test failures
- **Performance Regression**: Track performance metrics

## Test Documentation
- **Test Cases**: Document complex test scenarios
- **Setup Instructions**: Clear setup for running tests
- **Mock Documentation**: Document what each mock simulates
- **Test Data**: Document test data sources and management
